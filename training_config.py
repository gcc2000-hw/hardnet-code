"""
SPRING Hybrid Training Configuration (CORRECTED VERSION)
Defines the two-stage training pipeline for constraint-aware spatial reasoning.

CORRECTIONS MADE:
1. Updated DataConfig to use Telea backgrounds properly
2. Adjusted model config for ResNet18 scene encoder
3. Fixed Stage 1 config to use HybridSRM in discrete mode
4. Corrected loss configurations for SPRING methodology

Stage 1: Discrete Warm-up (Neural Foundation with HybridSRM)
Stage 2: Constraint-Aware Fine-tuning (Hybrid Reasoning)
"""

import torch
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Union
from pathlib import Path
from enum import Enum

# Import our constraint enforcement components
from spring_int import SpatialReasoningMode, HybridSRMConfig
from pipeline import LossScheduleStrategy, LossWeightConfig


class TrainingStage(Enum):
    STAGE1_DISCRETE = "stage1_discrete"
    STAGE2_CONSTRAINT = "stage2_constraint"


@dataclass
class DataConfig:
    """CORRECTED: Data loading and preprocessing configuration."""
    # COCO dataset paths (for annotations)
    train_data_path: str = "/home/gaurang/logicgrad/data/coco/train2017"
    val_data_path: str = "/home/gaurang/logicgrad/data/coco/val2017"
    train_annot_path: str = "/home/gaurang/logicgrad/data/coco/annotations/instances_train2017.json"
    val_annot_path: str = "/home/gaurang/logicgrad/data/coco/annotations/instances_val2017.json"
    
    # CORRECTED: Telea backgrounds (clean backgrounds for training)
    telea_train_dir: str = "./data/backgrounds/train"  # Generated by telea.py
    telea_val_dir: str = "./data/backgrounds/val"      # Generated by telea.py
    
    # Constraint data (will be used in Stage 2)
    constraint_data_path: str = "data/constraints"
    
    # Data processing
    batch_size_stage1: int = 8
    batch_size_stage2: int = 8
    num_workers: int = 2
    pin_memory: bool = True
    
    # CORRECTED: Image preprocessing (matching SPRING paper)
    image_size: int = 128  # SPRING uses 128x128 (not 512)
    normalize_coordinates: bool = True  # Use per-mille coordinates
    max_objects_per_scene: int = 5  # SPRING filters to â‰¤5 objects
    
    # Data augmentation (minimal for Stage 1, more for Stage 2)
    enable_augmentation: bool = True
    rotation_range: float = 5.0  # degrees (conservative for spatial reasoning)
    scale_range: float = 0.05    # small scale changes


@dataclass  
class ModelConfig:
    """CORRECTED: Model architecture configuration."""
    # Hybrid SRM settings (matching original SPRING)
    gru_hidden_size: int = 500
    gru_num_layers: int = 3
    gru_input_size: int = 4  # [x, y, width, height]
    dropout_rate: float = 0.1
    
    # CORRECTED: Scene encoding (ResNet18 backbone as in SPRING paper)
    scene_encoder_type: str = "resnet18"  # Match SPRING paper
    scene_encoder_dim: int = 512
    scene_encoder_pretrained: bool = True
    scene_to_layout_hidden: int = 1024  # Hidden layer for scene-to-layout projection
    
    # Constraint processing (Stage 2)
    max_constraints_per_batch: int = 1000
    constraint_timeout_seconds: float = 1.0
    enable_constraint_caching: bool = True
    
    # HardNet-Aff settings (CRITICAL: 100x regularization increase)
    hardnet_regularization: float = 1e-1  # CRITICAL: 100x increase for numerical stability
    hardnet_numerical_tolerance: float = 1e-6  # Practical tolerance for constraint satisfaction
    hardnet_coordinate_scale: float = 1.0   # FIXED: Normalized [0,1] coordinate system
    
    # Soft constraint settings
    temperature_initial: float = 2.0
    temperature_final: float = 0.01
    temperature_schedule: str = "exponential"


@dataclass
class Stage1Config:
    """CRITICAL FIX: Stage 1 - Extended warm-up training configuration."""
    # CRITICAL: Extended training for proper warm-up (was 21 epochs -> 100 epochs caused plateaus)
    epochs: int = 75  # Extended from 21 for proper neural foundation
    learning_rate: float = 1e-4  # Match SPRING paper
    weight_decay: float = 1e-5
    
    # CRITICAL: Learning rate scheduling to prevent plateaus
    enable_lr_scheduling: bool = True
    lr_scheduler: str = "cosine"  # Options: "cosine", "step", "exponential", "plateau"
    lr_min: float = 1e-6  # Minimum learning rate for cosine annealing
    warmup_epochs: int = 5  # Initial warmup period
    
    # CORRECTED: SRM mode (use HybridSRM in discrete mode)
    srm_mode: SpatialReasoningMode = SpatialReasoningMode.DISCRETE
    use_hybrid_srm: bool = True  # Use HybridSRM instead of simple model
    
    # CORRECTED: Loss configuration (proper SPRING losses)
    loss_weights: Dict[str, float] = field(default_factory=lambda: {
        'coordinate_loss': 1.0,     # Main coordinate regression loss
        'presence_loss': 0.1,       # Object presence learning
        'spatial_coherence': 0.05,  # Spatial relationship consistency
    })
    
    # Loss function type
    loss_function_type: str = "coordinate_regression"  # Not NLL for per-mille coords
    spatial_accuracy_tolerance: float = 50.0  # Per-mille tolerance (5% of image)
    
    # Optimization
    optimizer: str = "adam"
    gradient_clip_norm: float = 1.0
    accumulation_steps: int = 1
    
    # Validation and checkpointing
    val_every_n_epochs: int = 5
    save_every_n_epochs: int = 10
    early_stopping_patience: int = 15
    
    # Logging
    log_every_n_steps: int = 100
    tensorboard_log_dir: str = "logs/stage1"


@dataclass  
class Stage2Config:
    """Stage 2: Constraint-aware fine-tuning configuration."""
    # Training setup
    epochs: int = 200  # Extended for Phase 2 - more time for convergence after mathematical fixes
    learning_rate: float = 5e-5  # Lower for fine-tuning
    weight_decay: float = 1e-5
    scheduler: str = "cosine_with_restarts"
    
    # SRM mode
    srm_mode: SpatialReasoningMode = SpatialReasoningMode.HYBRID
    enable_mode_switching: bool = False  # FAIL-FAST: No fallback to discrete mode
    
    # Constraint enforcement schedule
    enable_hardnet_warm_start: bool = False  # FIXED: Disable warm-start to enable immediate projection
    hardnet_warm_start_epochs: int = 0  # No warm-start epochs needed
    constraint_curriculum_epochs: int = 40
    
    # CRITICAL: Balanced loss scheduling with normalized weights [0,10] range
    loss_configs: Dict[str, LossWeightConfig] = field(default_factory=lambda: {
        # Layout quality - PRODUCTION FIX: Normalize by 2000x to balance gradients
        'layout_quality': LossWeightConfig(
            initial_weight=0.0025,  # 5.0/2000 - normalized for ~36k loss magnitude
            final_weight=0.0015,    # 3.0/2000 - maintains emphasis ratio
            schedule_strategy=LossScheduleStrategy.COSINE,
            warmup_epochs=10
        ),
        
        # Hard constraint satisfaction - PRODUCTION FIX: Scale to 100 for balanced gradients
        'hardnet_constraint': LossWeightConfig(
            initial_weight=100.0,   # Balanced with layout loss
            final_weight=400.0,     # High emphasis on constraint satisfaction
            schedule_strategy=LossScheduleStrategy.CURRICULUM,
            schedule_params={
                'difficulty_stages': [0.2, 0.5, 0.8],
                'stage_weights': [50.0, 150.0, 300.0, 400.0],  # Progressive scaling
                'performance_threshold': 0.9,
                'min_epochs_per_stage': 15
            }
        ),
        
        # Soft constraint satisfaction - PRODUCTION FIX: Balanced scaling
        'soft_constraint': LossWeightConfig(
            initial_weight=100.0,  # Balanced with other components
            final_weight=200.0,    # Moderate final weight
            schedule_strategy=LossScheduleStrategy.ADAPTIVE,
            schedule_params={
                'adaptation_rate': 0.02,
                'target_satisfaction_rate': 0.95,
                'momentum': 0.9
            }
        ),
        
        # Constraint satisfaction regularization - PRODUCTION FIX: Balanced scaling
        'constraint_regularization': LossWeightConfig(
            initial_weight=50.0,   # Moderate regularization
            final_weight=100.0,    # Modest increase
            schedule_strategy=LossScheduleStrategy.LINEAR
        )
    })
    
    # Optimization with monitoring
    optimizer: str = "adamw"
    gradient_clip_norm: float = 0.5  # Tighter for constraint training
    accumulation_steps: int = 2  # Handle larger constraint batches
    
    # CRITICAL: Gradient monitoring and validation
    enable_gradient_monitoring: bool = True
    gradient_norm_threshold: float = 10.0  # Alert if gradients exceed this
    gradient_nan_check: bool = True  # FAIL-FAST on NaN gradients
    log_gradient_stats: bool = True  # Log gradient statistics
    
    # Constraint validation
    validate_constraints_every_n_epochs: int = 5
    constraint_satisfaction_threshold: float = 0.95
    
    # Advanced training techniques
    enable_mixed_precision: bool = False
    enable_gradient_checkpointing: bool = False
    
    # Validation and checkpointing
    val_every_n_epochs: int = 3  # More frequent validation
    save_every_n_epochs: int = 5
    early_stopping_patience: int = 20
    
    # Logging
    log_every_n_steps: int = 50  # More frequent logging
    tensorboard_log_dir: str = "logs/stage2"
    log_constraint_stats: bool = True
    log_constraint_violations: bool = True


@dataclass
class InfrastructureConfig:
    """Training infrastructure configuration."""
    # Device and performance
    device: str = "auto"  # "auto", "cuda", "cpu"
    mixed_precision: bool = False
    compile_model: bool = False
    
    # GPU optimization
    gradient_accumulation_steps: int = 1
    enable_gradient_checkpointing: bool = True  # Trade compute for memory
    max_memory_usage: float = 0.9  # Use 90% of available GPU memory
    
    # Reproducibility
    random_seed: int = 42
    deterministic: bool = False  # Set True for exact reproducibility
    
    # Checkpointing and resuming
    checkpoint_dir: str = "checkpoints"
    auto_resume: bool = True
    keep_n_checkpoints: int = 5
    
    # Monitoring and logging
    use_tensorboard: bool = True
    use_wandb: bool = False
    wandb_project: str = "spring-hybrid"
    
    # Performance monitoring
    profile_training: bool = False
    memory_profiling: bool = False
    
    # Error handling
    continue_on_error: bool = True
    max_retry_attempts: int = 3


@dataclass
class ExperimentConfig:
    """Experiment tracking and validation configuration."""
    # Experiment identification
    experiment_name: str = "spring_hybrid_corrected"
    experiment_version: str = "v1.0"
    tags: List[str] = field(default_factory=lambda: ["hybrid", "constraint-aware", "corrected"])
    
    # Evaluation metrics
    compute_fid_score: bool = True
    compute_is_score: bool = True
    constraint_satisfaction_metrics: bool = True
    
    # Validation datasets
    val_on_original_spring_data: bool = True
    val_on_constraint_benchmarks: bool = True
    
    # Export and visualization
    save_generated_samples: bool = True
    save_constraint_visualizations: bool = True
    export_model_for_demo: bool = True


@dataclass
class HybridTrainingConfig:
    """Complete two-stage training configuration."""
    # Sub-configurations
    data: DataConfig = field(default_factory=DataConfig)
    model: ModelConfig = field(default_factory=ModelConfig)
    stage1: Stage1Config = field(default_factory=Stage1Config)
    stage2: Stage2Config = field(default_factory=Stage2Config)
    infrastructure: InfrastructureConfig = field(default_factory=InfrastructureConfig)
    experiment: ExperimentConfig = field(default_factory=ExperimentConfig)
    
    # Global training settings
    total_epochs: int = 275  # stage1.epochs + stage2.epochs (75 + 200)
    enable_stage2: bool = True
    skip_stage1: bool = False  # Skip Stage 1 and train directly in Stage 2
    stage1_to_stage2_checkpoint: Optional[str] = None
    benchmark_mode: bool = False  # Enable PyTorch benchmarking mode
    detect_anomaly: bool = False  # Enable PyTorch anomaly detection
    checkpoint_dir: str = "checkpoints"  # Directory for saving checkpoints
    visualizations_dir: str = "visualizations"  # Directory for saving visualizations
    enable_wandb: bool = False  # Enable Weights & Biases logging
    dataset_root: str = "/home/gaurang/hardnet/data/spring_training_data"  # Path to SPRING training data
    
    def __post_init__(self):
        """Validate and adjust configuration after initialization."""
        # Ensure total epochs matches stages
        self.total_epochs = self.stage1.epochs + self.stage2.epochs
        
        # Adjust temperature schedule total epochs for stage2
        self.model.temperature_final = self.stage2.epochs
        
        # Create checkpoint directories
        Path(self.infrastructure.checkpoint_dir).mkdir(parents=True, exist_ok=True)
        Path(self.stage1.tensorboard_log_dir).mkdir(parents=True, exist_ok=True)
        Path(self.stage2.tensorboard_log_dir).mkdir(parents=True, exist_ok=True)
        
        # CORRECTED: Ensure Telea background directories exist
        Path(self.data.telea_train_dir).mkdir(parents=True, exist_ok=True)
        Path(self.data.telea_val_dir).mkdir(parents=True, exist_ok=True)
    
    def get_stage_config(self, stage: TrainingStage) -> Union[Stage1Config, Stage2Config]:
        """Get configuration for specific training stage."""
        if stage == TrainingStage.STAGE1_DISCRETE:
            return self.stage1
        elif stage == TrainingStage.STAGE2_CONSTRAINT:
            return self.stage2
        else:
            raise ValueError(f"Unknown training stage: {stage}")
    
    def get_hybrid_srm_config(self, stage: TrainingStage) -> HybridSRMConfig:
        """CORRECTED: Get HybridSRM configuration for specific stage."""
        stage_config = self.get_stage_config(stage)
        
        return HybridSRMConfig(
            mode=stage_config.srm_mode,
            fallback_to_discrete=False,  # FAIL-FAST: Always disabled
            
            # Neural network settings
            gru_hidden_size=self.model.gru_hidden_size,
            gru_num_layers=self.model.gru_num_layers,
            gru_input_size=self.model.gru_input_size,
            dropout_rate=self.model.dropout_rate,
            
            # Constraint settings
            enable_constraint_preprocessing=(stage == TrainingStage.STAGE2_CONSTRAINT),
            constraint_timeout_seconds=self.model.constraint_timeout_seconds,
            max_constraints_per_batch=self.model.max_constraints_per_batch,
            
            # HardNet settings
            hardnet_regularization=self.model.hardnet_regularization,
            hardnet_numerical_tolerance=self.model.hardnet_numerical_tolerance,
            enable_hardnet_warm_start=getattr(stage_config, 'enable_hardnet_warm_start', False),
            hardnet_warm_start_epochs=getattr(stage_config, 'hardnet_warm_start_epochs', 0),
            
            # Soft constraint settings
            enable_soft_constraints=(stage == TrainingStage.STAGE2_CONSTRAINT),
            temperature_initial=self.model.temperature_initial,
            temperature_final=self.model.temperature_final,
            temperature_schedule=self.model.temperature_schedule,
            temperature_total_epochs=stage_config.epochs,
            
            # Training settings
            enable_gradient_checkpointing=getattr(stage_config, 'enable_gradient_checkpointing', False),
            mixed_precision=getattr(stage_config, 'enable_mixed_precision', False),
            
            # Logging
            enable_logging=True,
            log_constraint_stats=getattr(stage_config, 'log_constraint_stats', False),
            log_performance_metrics=True
        )
    
    def validate_config(self) -> List[str]:
        """CORRECTED: Validate configuration and return any warnings."""
        warnings = []
        
        # Check if Telea background directories exist
        if not Path(self.data.telea_train_dir).exists():
            warnings.append(f"Telea training backgrounds not found: {self.data.telea_train_dir}")
        
        if not Path(self.data.telea_val_dir).exists():
            warnings.append(f"Telea validation backgrounds not found: {self.data.telea_val_dir}")
        
        # Check COCO data paths
        if not Path(self.data.train_data_path).exists():
            warnings.append(f"COCO training data not found: {self.data.train_data_path}")
        
        if not Path(self.data.train_annot_path).exists():
            warnings.append(f"COCO training annotations not found: {self.data.train_annot_path}")
        
        # Check Stage 1 configuration
        if self.stage1.srm_mode != SpatialReasoningMode.DISCRETE:
            warnings.append("Stage 1 should use DISCRETE mode for proper warm-up")
        
        # Check Stage 2 configuration
        if self.stage2.srm_mode not in [SpatialReasoningMode.DIFFERENTIABLE, SpatialReasoningMode.HYBRID]:
            warnings.append("Stage 2 should use DIFFERENTIABLE or HYBRID mode for constraint training")
        
        return warnings


# Predefined configurations for different scenarios
def get_quick_test_config() -> HybridTrainingConfig:
    """Quick configuration for testing (reduced epochs/batch sizes)."""
    config = HybridTrainingConfig()
    config.stage1.epochs = 5
    config.stage2.epochs = 10  
    config.data.batch_size_stage1 = 4
    config.data.batch_size_stage2 = 2
    config.experiment.experiment_name = "quick_test_corrected"
    return config


def get_research_config() -> HybridTrainingConfig:
    """Full research configuration (paper reproduction)."""
    config = HybridTrainingConfig()
    config.stage1.epochs = 100  # Match original SPRING
    config.stage2.epochs = 300  # Extended constraint training for research config
    config.data.batch_size_stage1 = 8   # SPRING paper setting
    config.data.batch_size_stage2 = 8   # SPRING paper setting
    config.experiment.experiment_name = "paper_reproduction_corrected"
    config.experiment.tags = ["research", "paper-reproduction", "full-scale", "corrected"]
    return config


def get_ablation_config(ablation_type: str) -> HybridTrainingConfig:
    """Configurations for ablation studies."""
    config = HybridTrainingConfig()
    config.experiment.experiment_name = f"ablation_{ablation_type}_corrected"
    
    if ablation_type == "no_constraints":
        # Disable all constraint enforcement
        config.stage2.srm_mode = SpatialReasoningMode.DISCRETE
        config.stage2.loss_configs = {
            'layout_quality': LossWeightConfig(initial_weight=1.0, final_weight=1.0)
        }
    elif ablation_type == "hard_only":
        # Only hard constraints, no soft
        config.stage2.loss_configs.pop('soft_constraint', None)
    elif ablation_type == "soft_only":
        # Only soft constraints, no hard
        config.stage2.loss_configs.pop('hardnet_constraint', None)
    elif ablation_type == "no_scene_encoder":
        # Test without ResNet18 scene encoder (use simple features)
        config.model.scene_encoder_type = "simple"
        config.model.scene_encoder_pretrained = False
    
    return config


def get_development_config() -> HybridTrainingConfig:
    """Configuration for development and debugging."""
    config = get_quick_test_config()
    config.infrastructure.profile_training = True
    config.infrastructure.memory_profiling = True
    config.experiment.experiment_name = "development_corrected"
    config.experiment.tags = ["development", "debugging", "corrected"]
    
    # More logging for development
    config.stage1.log_every_n_steps = 10
    config.stage2.log_every_n_steps = 10
    
    return config


if __name__ == "__main__":
    # Example usage and validation
    print("=== CORRECTED SPRING Hybrid Training Configuration ===\n")
    
    # Test configuration creation
    config = HybridTrainingConfig()
    print(f"Created CORRECTED training config:")
    print(f"  Stage 1: {config.stage1.epochs} epochs (mode: {config.stage1.srm_mode.value})")
    print(f"  Stage 2: {config.stage2.epochs} epochs (mode: {config.stage2.srm_mode.value})")
    print(f"  Total: {config.total_epochs} epochs")
    print(f"  Scene encoder: {config.model.scene_encoder_type}")
    print(f"  Image size: {config.data.image_size}x{config.data.image_size}")
    
    # Test stage-specific SRM configs
    srm_config_s1 = config.get_hybrid_srm_config(TrainingStage.STAGE1_DISCRETE)
    srm_config_s2 = config.get_hybrid_srm_config(TrainingStage.STAGE2_CONSTRAINT)
    
    print(f"\nStage 1 SRM: {srm_config_s1.mode.value}, Constraints: {srm_config_s1.enable_soft_constraints}")
    print(f"Stage 2 SRM: {srm_config_s2.mode.value}, Constraints: {srm_config_s2.enable_soft_constraints}")
    
    # Test predefined configs
    quick_config = get_quick_test_config()
    research_config = get_research_config()
    
    print(f"\nQuick test: {quick_config.stage1.epochs + quick_config.stage2.epochs} total epochs")
    print(f"Research: {research_config.stage1.epochs + research_config.stage2.epochs} total epochs")
    
    # Validate configuration
    warnings = config.validate_config()
    if warnings:
        print(f"\n  Configuration warnings:")
        for warning in warnings:
            print(f"  - {warning}")
    else:
        print(f"\n Configuration validation passed!")
    
    # Validate loss configuration
    print(f"\nStage 1 Loss Components:")
    for loss_name, weight in config.stage1.loss_weights.items():
        print(f"  {loss_name}: {weight}")
    
    print(f"\nStage 2 Loss Components:")
    for loss_name, loss_config in config.stage2.loss_configs.items():
        print(f"  {loss_name}: {loss_config.initial_weight} â†’ {loss_config.final_weight} ({loss_config.schedule_strategy.value})")
    
    print("\nâœ“ CORRECTED Configuration validation complete!")